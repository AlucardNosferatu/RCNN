from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense
from tensorflow.keras import Model
from RCNN import get_iou
import xml.etree.ElementTree as ElTr
import tensorflow as tf
import pandas as pd
import numpy as np
import pickle
import cv2
import os

path = "ProcessedData\\VOC2007_JPG"
annotation = "ProcessedData\\VOC2007_XML"

labels_dict = {
    "aeroplane": 1,
    "bicycle": 2,
    "bird": 3,
    "boat": 4,
    "bottle": 5,
    "bus": 6,
    "car": 7,
    "cat": 8,
    "chair": 9,
    "cow": 10,
    "diningtable": 11,
    "dog": 12,
    "horse": 13,
    "motorbike": 14,
    "person": 15,
    "pottedplant": 16,
    "sheep": 17,
    "sofa": 18,
    "train": 19,
    "tvmonitor": 20
}


def get_objects(file):
    tree = ElTr.parse(file)
    root = tree.getroot()
    objects = []
    for obj in root.iter('object'):
        objects.append(obj)
    return objects


def data_generator():
    file_size = 100
    train_images = []
    train_labels = []
    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()
    file_count = 0
    for e, annotation_file in enumerate(os.listdir(annotation)):
        filename = annotation_file.split(".")[0] + ".jpg"
        image = cv2.imread(os.path.join(path, filename))

        # region extract coordinates from annotation file
        objects = get_objects(os.path.join(annotation, annotation_file))
        gt_values = []
        gt_labels = []
        for obj in objects:
            name = obj.find('name').text
            gt_labels.append(name)
            xml_box = obj.find('bndbox')
            x1 = (float(xml_box.find('xmin').text) - 1)
            y1 = (float(xml_box.find('ymin').text) - 1)
            x2 = (float(xml_box.find('xmax').text) - 1)
            y2 = (float(xml_box.find('ymax').text) - 1)
            gt_values.append({"x1": x1, "x2": x2, "y1": y1, "y2": y2})
            # 把标签的坐标数据存入gt_values
        # endregion

        # region 加载SS ROI提出器
        ss.setBaseImage(image)
        ss.switchToSelectiveSearchFast()
        ss_results = ss.process()
        # endregion

        image_out = image.copy()
        counter = 0
        false_counter = 0
        flag = 0
        f_flag = 0
        b_flag = 0
        for e_roi, result in enumerate(ss_results):
            if e_roi < 2000 and flag == 0:
                print("batch_per_file: " + str(len(train_labels)))

                # region get iou list for each roi generated by SS
                iou_list = []
                for gt_value in gt_values:
                    x, y, w, h = result
                    iou = get_iou(gt_value, {"x1": x, "x2": x + w, "y1": y, "y2": y + h})
                    iou_list.append(iou)
                max_iou = max(iou_list)
                # 选择iou最大的那一个为可能的正样本
                index = iou_list.index(max_iou)
                # endregion

                # region P/N sample dispatcher
                if counter < 30:
                    # 选择交并比大于阈值的头30个候选坐标
                    if max_iou > 0.70:
                        # 交并比阈值0.7
                        target_image = image_out[y:y + h, x:x + w]
                        resized = cv2.resize(target_image, (224, 224), interpolation=cv2.INTER_AREA)
                        train_images.append(resized)
                        train_labels.append(labels_dict[gt_labels[index]])
                        # 根据labels_dict把字符串（例如‘dog’）映射到大于0的自然数（‘dog’→12）
                        counter += 1
                else:
                    f_flag = 1
                if false_counter < 30:
                    # IoU低于阈值0.3，前30个坐标作为负样本（背景）
                    if max_iou < 0.3:
                        target_image = image_out[y:y + h, x:x + w]
                        resized = cv2.resize(target_image, (224, 224), interpolation=cv2.INTER_AREA)
                        train_images.append(resized)
                        train_labels.append(0)
                        false_counter += 1
                else:
                    b_flag = 1
                # endregion

                if len(train_labels) >= file_size:
                    print("save to file now.")
                    ti_pkl = open('ProcessedData\\train_images_' + str(file_count) + '.pkl', 'wb')
                    tl_pkl = open('ProcessedData\\train_labels_' + str(file_count) + '.pkl', 'wb')
                    pickle.dump(train_images, ti_pkl)
                    pickle.dump(train_labels, tl_pkl)
                    ti_pkl.close()
                    tl_pkl.close()
                    file_count += 1
                    train_images = []
                    train_labels = []

                if f_flag == 1 and b_flag == 1:
                    # print("inside")
                    flag = 1

    ti_pkl = open('ProcessedData\\train_images_' + str(file_count) + '.pkl', 'wb')
    tl_pkl = open('ProcessedData\\train_labels_' + str(file_count) + '.pkl', 'wb')
    pickle.dump(train_images, ti_pkl)
    pickle.dump(train_labels, tl_pkl)
    ti_pkl.close()
    tl_pkl.close()


def data_loader():
    ti_pkl = open('ProcessedData\\train_images.pkl', 'rb')
    tl_pkl = open('ProcessedData\\train_labels.pkl', 'rb')
    train_images = pickle.load(ti_pkl)
    train_labels = pickle.load(tl_pkl)
    ti_pkl.close()
    tl_pkl.close()
    x_new = np.array(train_images)
    y_new = np.array(train_labels)
    return x_new, y_new


def transfer_model_build():
    model_loaded = tf.keras.models.load_model("TrainedModels\\RCNN.h5")
    x = model_loaded.layers[21].output
    x = Dense(21, activation='softmax')(x)
    model_final = Model(inputs=model_loaded.input, outputs=x)
    opt = Adam(lr=0.0001)
    model_final.compile(
        loss=tf.keras.losses.CategoricalCrossentropy(),
        optimizer=opt,
        metrics=["accuracy"]
    )
    model_final.summary()
    model_final.save("TrainedModels\\RCNN-VOC2007.h5")


def transfer_model_train():
    model_final = tf.keras.models.load_model("TrainedModels\\RCNN-VOC2007.h5")


data_generator()
